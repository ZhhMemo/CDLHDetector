{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "from MyVisitor import MyVisitor\n",
    "\n",
    "from pycparser import c_parser, c_ast, parse_file\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "def preprocess():\n",
    "    \"\"\"\n",
    "    Preprocess datasets. Get all ASTs.\n",
    "\n",
    "    Return:\n",
    "        (training_data, test_data)\n",
    "    \"\"\"\n",
    "    training_data = []\n",
    "    test_data = []\n",
    "    training_dir = './train'\n",
    "    test_dir = './test'\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(training_dir):\n",
    "        if '.DS_Store' in filenames:\n",
    "            filenames.remove('.DS_Store')\n",
    "        for filename in filenames:\n",
    "            #print(os.path.join(dirpath, filename))\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            try:\n",
    "                ast = parse_file(filepath, use_cpp=False)\n",
    "                cv = MyVisitor()\n",
    "                cv.visit(ast)\n",
    "                training_data.append(cv.values)\n",
    "            except Exception as e:\n",
    "                print('error when parsing', filepath, \"type:\", type(e))\n",
    "                training_data.append([])\n",
    "    #save training_data into file\n",
    "    with open('training_data.txt', 'w') as f:\n",
    "        pickle.dump(training_data, f)\n",
    "    print(\"training_data length: \", len(training_data))\n",
    "    print(\"training_data[0]:\", training_data[0])\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(test_dir):\n",
    "        if '.DS_Store' in filenames:\n",
    "            filenames.remove('.DS_Store')\n",
    "        for filename in filenames:\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            try:\n",
    "                ast = parse_file(filepath, use_cpp=False)\n",
    "                cv = MyVisitor()\n",
    "                cv.visit(ast)\n",
    "                test_data.append(cv.values)\n",
    "            except:\n",
    "                print('error when parsing', filepath)\n",
    "    #save test_data into files\n",
    "    with open('test_data.txt', 'w') as f:\n",
    "        pickle.dump(test_data, f)\n",
    "    print(\"test_data length: \", len(test_data))\n",
    "    print(\"test_data[0]:\", test_data[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "test_data = []\n",
    "training_dir = './train'\n",
    "test_dir = './test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dirpath, dirnames, filenames in os.walk(test_dir):\n",
    "        if '.DS_Store' in filenames:\n",
    "            filenames.remove('.DS_Store')\n",
    "        for filename in filenames:\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            try:\n",
    "                ast = parse_file(filepath, use_cpp=False)\n",
    "                cv = MyVisitor()\n",
    "                cv.visit(ast)\n",
    "                test_data.append(cv.values)\n",
    "            except:\n",
    "                print('error when parsing', filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_data.txt', 'wb') as f:\n",
    "        pickle.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<ipython-input-14-c6666be53f30>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-c6666be53f30>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    with open('training_data.txt', 'wb') as f:\u001b[0m\n\u001b[0m                                              ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "for dirpath, dirnames, filenames in os.walk(training_dir):\n",
    "        if '.DS_Store' in filenames:\n",
    "            filenames.remove('.DS_Store')\n",
    "        for filename in filenames:\n",
    "            #print(os.path.join(dirpath, filename))\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            try:\n",
    "                ast = parse_file(filepath, use_cpp=False)\n",
    "                cv = MyVisitor()\n",
    "                cv.visit(ast)\n",
    "                training_data.append(cv.values)\n",
    "            except Exception as e:\n",
    "                print('error when parsing', filepath, \"type:\", type(e))\n",
    "                training_data.append([])\n",
    "    #save training_data into file\n",
    "    with open('training_data.txt', 'wb') as f:\n",
    "        pickle.dump(training_data, f)\n",
    "    print(\"training_data length: \", len(training_data))\n",
    "    print(\"training_data[0]:\", training_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error when parsing ./train/7798/9c6fd0076f35430a.txt type: <class 'pycparser.plyparser.ParseError'>\n",
      "error when parsing ./train/9431/bb2c0fd45b3e49b7.txt type: <class 'pycparser.plyparser.ParseError'>\n",
      "error when parsing ./train/9431/88b3d8cbdb974cfa.txt type: <class 'pycparser.plyparser.ParseError'>\n",
      "error when parsing ./train/998c/c79c415926e14962.txt type: <class 'pycparser.plyparser.ParseError'>\n",
      "error when parsing ./train/e51d/99c963f357924982.txt type: <class 'pycparser.plyparser.ParseError'>\n",
      "training_data length:  41500\n",
      "training_data[0]: ['main', 'int', 'VAR_1238123', 'int', 'VAR__8444594', 'int', 'VAR__53647', 'int', 'VAR__21230', 'int', 'VAR__1111', 'int', 'VAR__819283', 'int', 'VAR__SUM', 'int', '0', 'VAR__127322', 'int', '0', 'VAR__1238129', 'int', '1000', 'scanf', '\"%d%d\"', '&', 'VAR_1238123', '&', 'VAR__8444594', 'for', '=', 'VAR__53647', '1', '<=', 'VAR__53647', 'VAR_1238123', 'p++', 'VAR__53647', 'scanf', '\"%d\"', '&', 'VAR__819283', '=', 'VAR__1238129', 'VAR__53647', 'VAR__819283', 'for', '=', 'VAR__21230', '1', '<=', 'VAR__21230', 'VAR_1238123', 'p++', 'VAR__21230', 'for', '=', 'VAR__1111', 'VAR__21230', '<=', 'VAR__1111', 'VAR_1238123', 'p++', 'VAR__1111', '=', 'VAR__SUM', '+', 'VAR__1238129', 'VAR__21230', 'VAR__1238129', 'VAR__1111', 'if', '==', 'VAR__SUM', 'VAR__8444594', '=', 'VAR__127322', '+', 'VAR__127322', '1', '=', 'VAR__127322', 'VAR__127322', 'if', '==', 'VAR__127322', '0', 'printf', '\"no\"', 'printf', '\"yes\"', 'return', '0']\n"
     ]
    }
   ],
   "source": [
    "for dirpath, dirnames, filenames in os.walk(training_dir):\n",
    "    if '.DS_Store' in filenames:\n",
    "        filenames.remove('.DS_Store')\n",
    "    for filename in filenames:\n",
    "        #print(os.path.join(dirpath, filename))\n",
    "        filepath = os.path.join(dirpath, filename)\n",
    "        try:\n",
    "            ast = parse_file(filepath, use_cpp=False)\n",
    "            cv = MyVisitor()\n",
    "            cv.visit(ast)\n",
    "            training_data.append(cv.values)\n",
    "        except Exception as e:\n",
    "            print('error when parsing', filepath, \"type:\", type(e))\n",
    "            training_data.append([])\n",
    "#save training_data into file\n",
    "with open('training_data.txt', 'wb') as f:\n",
    "    pickle.dump(training_data, f)\n",
    "print(\"training_data length: \", len(training_data))\n",
    "print(\"training_data[0]:\", training_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {}\n",
    "for sentence in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "for sentence in test_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14750"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, target_size):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        ##LSTM 以 word_embeddings 作为输入, 输出维度为 hidden_dim 的隐状态值\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden = self.init_hidden()\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)), \n",
    "                autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        tag_scores = F.log_softmax(lstm_out.view(len(sentence), -1), dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModule(100, 83, 1000, 83)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type MyModule. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, 'model-s.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
